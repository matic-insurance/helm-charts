global:
  # Default values for application charts.
  application:
    # Product name
    product: ""
    # Application environment that it is running in
    environment: ""
    # Application version
    version: ""

  # Global application docker image configuration
  applicationImage:
    # docker images repo
    repository: ""
    # How image should be pulled from registry
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the Values.version.
    tag: ""
    # Custom secret to authenticate pulling from private registries
    pullSecret: ""

  # Global configuration for application monitoring tools
  applicationMonitoring:
    # Enable datadog monitoring and include additional labels to deployment
    datadog: false
    # Provision environment variables for Sentry exception tracking
    sentry: false

  # Global mesh configurations
  mesh:
    # Kill switch for mesh deployment. Should be set to false to deploy application without service mesh configurations
    enabled: true
    # Allows to disable istio sidecar injection and command wrapping
    injectionEnabled: true

# Current Application component that is deployed. Impacts k8s naming/labels
component: ""

# Command to execute inside of container `bundle exec puma`
command: ""

# availability configures different aspects of application replication, disruption budget and updates
# availability:critical
#  - checks if app has at least 3 pods,
#  - update/disruption allow at most one pod as unhealthy
#  - shutdown have additional pre stop hook delay to allow other services stop using pod that is deleted
#  - affinity demands placing pods on different hosts
# availability:high
#  - checks app to have at least 2 pods
#  - update/disruption allow at most one pod as unhealthy
#  - shutdown have additional pre stop hook delay to allow other services stop using pod that is deleted
#  - affinity asks placing pods on different hosts
# availability:normal
#  - checks app to have at least 2 pods
#  - update/disruption allow at most one pod as unhealthy
# availability:irrelevant
#  - it is ok for component to be down for short period of time.
availability: irrelevant

# placement tells where pods should be scheduled: spot, on_demand, mixed, irrelevant
# configuration should not be mixed with affinity/topologySpreadConstraints/tolerations
# By default no special placement is done
placement: irrelevant

# resources describe what pod resources should have.
# Supports "low" "normal" "high" predefined values
# Supports k8s definition if custom values are needed
resources: normal

# Component application image configuration. Overrides global configuration
applicationImage:
  # docker images repo
  repository: ""
  # Overrides the image tag whose default is the Values.version.
  tag: ""

# Deployment configurations
deployment:
  # Number of pods to run. Ignored when autoscaling enabled
  replicas: 2

  # port configures exposed port by deployment and used in service and ingress configurations.
  port: 9292

  # resources allow to customize dedicated resources. overrides .Values.resources
  resources: {}

  # Additional environment variables in k8s format:
  # - name: TEST
  #   value: true
  env: []

  # Additional environment variables to mount from config map or secrets:
  # - secretRef:
  #   name: test-secret-envvars
  envFrom: []

  # Additional volumes to mount to container
  # - name: cache-vol
  #   mountPath: /app/cache
  volumeMounts: []

  # Additional volumes to add to the pod
  # - name: cache-vol
  #   emptyDir: {}
  volumes: []

  # Custom annotations to be added to the pod
  annotations: {}

  # lifecycle configuration for container
  lifecycle: {}

  # Advanced deployment options
  # How pods resolve DNS records
  dnsPolicy: ClusterFirst

  # Custom DNS configuration. Overrides default ndots 1 option
  dnsConfig:

  # Security context configuration for pod.
  securityContext:
    fsGroup: 1000
    runAsUser: 1000

  # Time before deployment starts checking for pod lifecycle
  minReadySeconds: 1
  # Time after which deployment considered failed if no progress is made
  progressDeadlineSeconds: 120
  # How long k8s waits for pod to terminate
  terminationGracePeriodSeconds: 30
  # How many old ReplicaSets for this Deployment you want to retain
  revisionHistoryLimit: 5
  # strategy - configures how pod are updated.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 3
      maxUnavailable: 0

  # Scheduling options
  nodeSelector: {}
  tolerations: []
  affinity: {}
  topologySpreadConstraints: []

  # logs configure logging for pod
  logs:
    # logs.source tells datadog how to parse logs from this deployment
    source: ruby

# Service configuration for the pods
service:
  # Should we create the service
  enabled: false
  # Type of the service
  type: ClusterIP
  # Port exposed by service
  port: 80

# Ingress configuration
# Ingress will forward all requests to the service configured above. Ingress is not created when service disabled
ingress:
  enabled: false
  # List of ingress resources to create
  list:
      # Domain for ingress
    - host: myhost1.local
      # List of additional domains to support by this ingress
      additional_hosts: []
      # Type of ingress: external(public)/internal(private)
      type: external
      # List of allowed locations. Default "/"
      allow_locations:
        - /api
        - /upload
      # List of locations to block. Default empty
      deny_locations:
        - /admin
        - /internal_api
        - /etc
      # Secret that have certificate information. Otherwise it tls will be disabled
      tls: myhost1.local-tls
      # Additional annotations for ingress
      annotations: {}
      # Custom nginx snippets
      server_snippet: |
        location /foldername-a {
          proxy_http_version 1.1;
          proxy_set_header X-Forwarded-Proto $scheme;
        }
        location /foldername-b {
          proxy_set_header X-Forwarded-Host $http_host;
        }

# probes hold configuration for all probes used by pod
probes:
  enabled: false
  # probe types (all, startup, readiness). short hand to dictates what probe are created in addition to enabled tag
  type: all

  # Pod health probes configurations
  # Check if pod successfully started
  # Passing probe - will mark pod as alive and start sending traffic to it.
  # Failing probe - pod killed and new one will be started
  # Default configuration designed to wait up to 3 minutes for pod to start responding
  startupProbe:
    enabled: true
    # How health will be evaluated.
    path: /healthz/webserver
    # Specify command if you want probe to use exec
    command:
    # Specify port if you want to override default deployment port
    #    port: 3000
    # How long to wait before sending first probe
    initialDelaySeconds: 1
    # What is the timeout of request
    timeoutSeconds: 5
    # What is the delay between requests
    periodSeconds: 5
    # How many checks should be successful to mark pod alive
    successThreshold: 1
    # How many checks before pod considered dead
    failureThreshold: 40
  # Check if pod is ready to accept traffic.
  # This probe does not restart the pod but simply removes it from accepting additional traffic
  # Intention of readiness probe is to temporary remove pod from balancer if it was overloaded by requests
  # Default configuration check if pod is not responding during one minute and removes it from balancer
  readinessProbe:
    enabled: true
    path: /healthz/webserver
    # Specify command if you want probe to use exec
    command:
    initialDelaySeconds: 0
    timeoutSeconds: 10
    periodSeconds: 15
    successThreshold: 1
    failureThreshold: 4
  # Check if pod is alive.
  # If pod is not alive - pod is replaced with new pod
  # Default configuration checks if pod is not responding for 2 minutes and kills it
  # Liveness probe has bigger timeouts and periods so k8s first remove pod from balancer and if it is not recovered - kills it.
  livenessProbe:
    enabled: true
    path: /healthz/webserver
    # Specify command if you want probe to use exec
    command:
    initialDelaySeconds: 60
    timeoutSeconds: 20
    periodSeconds: 30
    successThreshold: 1
    failureThreshold: 4

# Autoscaling configuration
autoscaling:
  enabled: false
  # Maximum number of pods to scale
  maxReplicas: 6
  # Min number of pods to scale
  minReplicas: 3
  # Metrics that used to evaluate target pods
  metrics: []
    # Example datadog metrics
#    - name: origin-p95-latency
#      type: Datadog
#      query: p95:trace.nginx.handle{env:production,resource_name:/,service:origin-nginx}.rollup(avg, 60)
#      target:
#        type: Value # desiredReplicas = ceil[currentReplicas * ( currentMetricValue / desiredMetricValue )] - number of pods should double if latency is 1 second
#        value: 500m # p95 nginx latency goes above 500ms
#    - name: origin-requests-per-second
#      type: Datadog
#      query: ewma_10(avg:nginx_ingress.controller.requests{service:origin-rails}.as_count().rollup(avg, 60))
#      target:
#        type: AverageValue # desiredReplicas = ceil[( currentMetricValue / desiredMetricValue )] - if nginx receives 60 requests per second - it should have 5 pods to process them
#        averageValue: 12 # Assuming every thread processing 2 requests per second, and every pod has 6 threads
#    - type: Resource
#      resource:
#        name: cpu
#        target:
#          type: Utilization
#          averageUtilization: 60
#    - type: External
#      external:
#        metric:
#          name: sidekiq.queue.size
#          selector:
#            matchLabels:
#              product: servicing
#              environment: production
#        target:
#          type: Value
#          value: 2000

  # Autoscaling behaviour
  behavior:
    scaleUp:
      # Scale up target selected rolling max based on rolling max of last 15 seconds
      stabilizationWindowSeconds: 15
      policies:
        # Allows to scale up 2 pods at most every minute (periodSeconds)
        - type: Pods
          value: 2
          periodSeconds: 60
    scaleDown:
      # Scale down target selected based on rolling max of last minute
      stabilizationWindowSeconds: 60
      policies:
        # Allows to scale down 1 pods at most every 3 minutes (periodSeconds)
        - type: Pods
          value: 1
          periodSeconds: 180

serviceAccount:
  # Specifies whether a service account should be created
  create: false
  # The name of the service account to use.
  # If not set a name is generated using the name template
  name: ""
  # Additional Annotations to add to the service account when creating
  annotations: {}

mesh:
  # Configuration to orchestrate correct start/stop of components in a mesh
  lifecycle:
    enabled: true
    # Wait for Istio service to be available, before running container command
    startupWaitCommand: "until wget --quiet --spider -o /dev/null http://localhost:15020/healthz/ready; do echo 'Waiting for Istio...'; sleep 1; done; echo 'Istio ready. Running the command...';"
    # preStopHook to execute on shutdown
    shutdownWaitCommand: "echo 'Waiting for Application exit'; kill 1; wait 1; echo 'Application finished. Telling Istio to exit'; wget -quiet --post-data 'exit' -o /dev/null http://localhost:15020/quitquitquit"


# default contain default values for various resources
defaults:
  # ingress contain default values for ingress resource
  ingress:
    allow_locations:
      - /
    additional_hosts: []
    annotations:
      nginx.ingress.kubernetes.io/load-balance: "ewma"
      nginx.ingress.kubernetes.io/ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384"
      nginx.ingress.kubernetes.io/proxy-next-upstream: error timeout non_idempotent
      nginx.ingress.kubernetes.io/proxy-next-upstream-tries: 8
      nginx.ingress.kubernetes.io/proxy-body-size: 256m
      external-dns.alpha.kubernetes.io/ttl: "60"

  # default resource configurations
  resources:
    # Low utilization resources
    low:
      # Minimal requirements for pod
      requests:
        memory: 200Mi
        cpu: 200m
      # Maximum resources for pod
      limits:
        memory: 700Mi
        cpu: 600m
    # Normal app resources
    normal:
      # Minimal requirements for pod
      requests:
        memory: 500Mi
        cpu: 400m
      # Maximum resources for pod
      limits:
        memory: 1500Mi
        cpu: 1200m
    # heavy app resources
    high:
      # Minimal requirements for pod
      requests:
        memory: 700Mi
        cpu: 600m
      # Maximum resources for pod
      limits:
        memory: 2100Mi
        cpu: 1800m

  affinity:
    # spot placement prefers all pods to be scheduled to spot instances
    spot:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: eks.amazonaws.com/capacityType
                  operator: In
                  values:
                    - Ec2Spot
    # on demand placement - prefers all pods on on demand instances instances
    on_demand:
      nodeAffinity:
        preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
                - key: eks.amazonaws.com/capacityType
                  operator: In
                  values:
                    - ON_DEMAND
    # Mixed placement do not have any affinity but using topologySpreadConstraints
    mixed:
    irrelevant:

  topologySpreadConstraints:
    critical:
      # requires even distribution across hosts
      # asks pods to be distributed across zones
      spot:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      # requires even distribution across hosts
      # asks pods to be distributed across zones
      on_demand:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      # requires even distribution across node type and hosts
      # asks pods to be distributed across zones
      mixed:
        - maxSkew: 1
          topologyKey: eks.amazonaws.com/capacityType
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      irrelevant:

    high:
      # requires uneven distribution across hosts
      # asks pods to be distributed across zones
      spot:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      # requires uneven distribution across hosts
      # asks pods to be distributed across zones
      on_demand:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      # requires uneven distribution across node type and hosts
      # asks pods to be distributed across zones
      mixed:
        - maxSkew: 2
          topologyKey: eks.amazonaws.com/capacityType
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      irrelevant:

    normal:
      # asks uneven distribution across hosts and zones
      spot:
        - maxSkew: 2
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      # asks uneven distribution across hosts and zones
      on_demand:
        - maxSkew: 2
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      # requires uneven distribution across node types
      # asks pods to be more or less evenly across hosts and zones
      mixed:
        - maxSkew: 2
          topologyKey: eks.amazonaws.com/capacityType
          whenUnsatisfiable: DoNotSchedule
        - maxSkew: 2
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
      irrelevant:

    irrelevant:
      spot:
      on_demand:
      # asks uneven distribution across node types
      mixed:
        - maxSkew: 2
          topologyKey: eks.amazonaws.com/capacityType
          whenUnsatisfiable: ScheduleAnyway
      irrelevant:

  # Default DNS configuration for application deployment
  dnsConfig:
    options:
      - name: ndots
        value: "1"
